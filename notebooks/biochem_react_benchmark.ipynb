{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pika.main import Pika\n",
    "from pika.utils.helpers import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# prep config\n",
    "assets_path = \"../tests/assets/\"\n",
    "config = load_config(assets_path + \"sample_self_pika_config.json\")\n",
    "config[\"datamodule\"][\"data_dict_path\"] = assets_path + \"sample_data.pkl\"\n",
    "config[\"datamodule\"][\"split_path\"] = assets_path + \"sample_split.csv\"\n",
    "config[\"datamodule\"][\"test_subjects\"] = [\"reaction\", \"taxonomy\"]\n",
    "config[\"model\"][\"checkpoint\"] = \"../model_checkpoints/self_pika_gpt2m_esm2m.ckpt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 7\n",
      "Using cache found in /Users/elicarrami/.cache/torch/hub/facebookresearch_esm_main\n",
      "/Users/elicarrami/opt/anaconda3/envs/pika/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:173: Found keys that are in the model state dict but not in the checkpoint: ['esm.embed_tokens.weight', 'esm.layers.0.self_attn.k_proj.weight', 'esm.layers.0.self_attn.k_proj.bias', 'esm.layers.0.self_attn.v_proj.weight', 'esm.layers.0.self_attn.v_proj.bias', 'esm.layers.0.self_attn.q_proj.weight', 'esm.layers.0.self_attn.q_proj.bias', 'esm.layers.0.self_attn.out_proj.weight', 'esm.layers.0.self_attn.out_proj.bias', 'esm.layers.0.self_attn.rot_emb.inv_freq', 'esm.layers.0.self_attn_layer_norm.weight', 'esm.layers.0.self_attn_layer_norm.bias', 'esm.layers.0.fc1.weight', 'esm.layers.0.fc1.bias', 'esm.layers.0.fc2.weight', 'esm.layers.0.fc2.bias', 'esm.layers.0.final_layer_norm.weight', 'esm.layers.0.final_layer_norm.bias', 'esm.layers.1.self_attn.k_proj.weight', 'esm.layers.1.self_attn.k_proj.bias', 'esm.layers.1.self_attn.v_proj.weight', 'esm.layers.1.self_attn.v_proj.bias', 'esm.layers.1.self_attn.q_proj.weight', 'esm.layers.1.self_attn.q_proj.bias', 'esm.layers.1.self_attn.out_proj.weight', 'esm.layers.1.self_attn.out_proj.bias', 'esm.layers.1.self_attn.rot_emb.inv_freq', 'esm.layers.1.self_attn_layer_norm.weight', 'esm.layers.1.self_attn_layer_norm.bias', 'esm.layers.1.fc1.weight', 'esm.layers.1.fc1.bias', 'esm.layers.1.fc2.weight', 'esm.layers.1.fc2.bias', 'esm.layers.1.final_layer_norm.weight', 'esm.layers.1.final_layer_norm.bias', 'esm.layers.2.self_attn.k_proj.weight', 'esm.layers.2.self_attn.k_proj.bias', 'esm.layers.2.self_attn.v_proj.weight', 'esm.layers.2.self_attn.v_proj.bias', 'esm.layers.2.self_attn.q_proj.weight', 'esm.layers.2.self_attn.q_proj.bias', 'esm.layers.2.self_attn.out_proj.weight', 'esm.layers.2.self_attn.out_proj.bias', 'esm.layers.2.self_attn.rot_emb.inv_freq', 'esm.layers.2.self_attn_layer_norm.weight', 'esm.layers.2.self_attn_layer_norm.bias', 'esm.layers.2.fc1.weight', 'esm.layers.2.fc1.bias', 'esm.layers.2.fc2.weight', 'esm.layers.2.fc2.bias', 'esm.layers.2.final_layer_norm.weight', 'esm.layers.2.final_layer_norm.bias', 'esm.layers.3.self_attn.k_proj.weight', 'esm.layers.3.self_attn.k_proj.bias', 'esm.layers.3.self_attn.v_proj.weight', 'esm.layers.3.self_attn.v_proj.bias', 'esm.layers.3.self_attn.q_proj.weight', 'esm.layers.3.self_attn.q_proj.bias', 'esm.layers.3.self_attn.out_proj.weight', 'esm.layers.3.self_attn.out_proj.bias', 'esm.layers.3.self_attn.rot_emb.inv_freq', 'esm.layers.3.self_attn_layer_norm.weight', 'esm.layers.3.self_attn_layer_norm.bias', 'esm.layers.3.fc1.weight', 'esm.layers.3.fc1.bias', 'esm.layers.3.fc2.weight', 'esm.layers.3.fc2.bias', 'esm.layers.3.final_layer_norm.weight', 'esm.layers.3.final_layer_norm.bias', 'esm.layers.4.self_attn.k_proj.weight', 'esm.layers.4.self_attn.k_proj.bias', 'esm.layers.4.self_attn.v_proj.weight', 'esm.layers.4.self_attn.v_proj.bias', 'esm.layers.4.self_attn.q_proj.weight', 'esm.layers.4.self_attn.q_proj.bias', 'esm.layers.4.self_attn.out_proj.weight', 'esm.layers.4.self_attn.out_proj.bias', 'esm.layers.4.self_attn.rot_emb.inv_freq', 'esm.layers.4.self_attn_layer_norm.weight', 'esm.layers.4.self_attn_layer_norm.bias', 'esm.layers.4.fc1.weight', 'esm.layers.4.fc1.bias', 'esm.layers.4.fc2.weight', 'esm.layers.4.fc2.bias', 'esm.layers.4.final_layer_norm.weight', 'esm.layers.4.final_layer_norm.bias', 'esm.layers.5.self_attn.k_proj.weight', 'esm.layers.5.self_attn.k_proj.bias', 'esm.layers.5.self_attn.v_proj.weight', 'esm.layers.5.self_attn.v_proj.bias', 'esm.layers.5.self_attn.q_proj.weight', 'esm.layers.5.self_attn.q_proj.bias', 'esm.layers.5.self_attn.out_proj.weight', 'esm.layers.5.self_attn.out_proj.bias', 'esm.layers.5.self_attn.rot_emb.inv_freq', 'esm.layers.5.self_attn_layer_norm.weight', 'esm.layers.5.self_attn_layer_norm.bias', 'esm.layers.5.fc1.weight', 'esm.layers.5.fc1.bias', 'esm.layers.5.fc2.weight', 'esm.layers.5.fc2.bias', 'esm.layers.5.final_layer_norm.weight', 'esm.layers.5.final_layer_norm.bias', 'esm.layers.6.self_attn.k_proj.weight', 'esm.layers.6.self_attn.k_proj.bias', 'esm.layers.6.self_attn.v_proj.weight', 'esm.layers.6.self_attn.v_proj.bias', 'esm.layers.6.self_attn.q_proj.weight', 'esm.layers.6.self_attn.q_proj.bias', 'esm.layers.6.self_attn.out_proj.weight', 'esm.layers.6.self_attn.out_proj.bias', 'esm.layers.6.self_attn.rot_emb.inv_freq', 'esm.layers.6.self_attn_layer_norm.weight', 'esm.layers.6.self_attn_layer_norm.bias', 'esm.layers.6.fc1.weight', 'esm.layers.6.fc1.bias', 'esm.layers.6.fc2.weight', 'esm.layers.6.fc2.bias', 'esm.layers.6.final_layer_norm.weight', 'esm.layers.6.final_layer_norm.bias', 'esm.layers.7.self_attn.k_proj.weight', 'esm.layers.7.self_attn.k_proj.bias', 'esm.layers.7.self_attn.v_proj.weight', 'esm.layers.7.self_attn.v_proj.bias', 'esm.layers.7.self_attn.q_proj.weight', 'esm.layers.7.self_attn.q_proj.bias', 'esm.layers.7.self_attn.out_proj.weight', 'esm.layers.7.self_attn.out_proj.bias', 'esm.layers.7.self_attn.rot_emb.inv_freq', 'esm.layers.7.self_attn_layer_norm.weight', 'esm.layers.7.self_attn_layer_norm.bias', 'esm.layers.7.fc1.weight', 'esm.layers.7.fc1.bias', 'esm.layers.7.fc2.weight', 'esm.layers.7.fc2.bias', 'esm.layers.7.final_layer_norm.weight', 'esm.layers.7.final_layer_norm.bias', 'esm.layers.8.self_attn.k_proj.weight', 'esm.layers.8.self_attn.k_proj.bias', 'esm.layers.8.self_attn.v_proj.weight', 'esm.layers.8.self_attn.v_proj.bias', 'esm.layers.8.self_attn.q_proj.weight', 'esm.layers.8.self_attn.q_proj.bias', 'esm.layers.8.self_attn.out_proj.weight', 'esm.layers.8.self_attn.out_proj.bias', 'esm.layers.8.self_attn.rot_emb.inv_freq', 'esm.layers.8.self_attn_layer_norm.weight', 'esm.layers.8.self_attn_layer_norm.bias', 'esm.layers.8.fc1.weight', 'esm.layers.8.fc1.bias', 'esm.layers.8.fc2.weight', 'esm.layers.8.fc2.bias', 'esm.layers.8.final_layer_norm.weight', 'esm.layers.8.final_layer_norm.bias', 'esm.layers.9.self_attn.k_proj.weight', 'esm.layers.9.self_attn.k_proj.bias', 'esm.layers.9.self_attn.v_proj.weight', 'esm.layers.9.self_attn.v_proj.bias', 'esm.layers.9.self_attn.q_proj.weight', 'esm.layers.9.self_attn.q_proj.bias', 'esm.layers.9.self_attn.out_proj.weight', 'esm.layers.9.self_attn.out_proj.bias', 'esm.layers.9.self_attn.rot_emb.inv_freq', 'esm.layers.9.self_attn_layer_norm.weight', 'esm.layers.9.self_attn_layer_norm.bias', 'esm.layers.9.fc1.weight', 'esm.layers.9.fc1.bias', 'esm.layers.9.fc2.weight', 'esm.layers.9.fc2.bias', 'esm.layers.9.final_layer_norm.weight', 'esm.layers.9.final_layer_norm.bias', 'esm.layers.10.self_attn.k_proj.weight', 'esm.layers.10.self_attn.k_proj.bias', 'esm.layers.10.self_attn.v_proj.weight', 'esm.layers.10.self_attn.v_proj.bias', 'esm.layers.10.self_attn.q_proj.weight', 'esm.layers.10.self_attn.q_proj.bias', 'esm.layers.10.self_attn.out_proj.weight', 'esm.layers.10.self_attn.out_proj.bias', 'esm.layers.10.self_attn.rot_emb.inv_freq', 'esm.layers.10.self_attn_layer_norm.weight', 'esm.layers.10.self_attn_layer_norm.bias', 'esm.layers.10.fc1.weight', 'esm.layers.10.fc1.bias', 'esm.layers.10.fc2.weight', 'esm.layers.10.fc2.bias', 'esm.layers.10.final_layer_norm.weight', 'esm.layers.10.final_layer_norm.bias', 'esm.layers.11.self_attn.k_proj.weight', 'esm.layers.11.self_attn.k_proj.bias', 'esm.layers.11.self_attn.v_proj.weight', 'esm.layers.11.self_attn.v_proj.bias', 'esm.layers.11.self_attn.q_proj.weight', 'esm.layers.11.self_attn.q_proj.bias', 'esm.layers.11.self_attn.out_proj.weight', 'esm.layers.11.self_attn.out_proj.bias', 'esm.layers.11.self_attn.rot_emb.inv_freq', 'esm.layers.11.self_attn_layer_norm.weight', 'esm.layers.11.self_attn_layer_norm.bias', 'esm.layers.11.fc1.weight', 'esm.layers.11.fc1.bias', 'esm.layers.11.fc2.weight', 'esm.layers.11.fc2.bias', 'esm.layers.11.final_layer_norm.weight', 'esm.layers.11.final_layer_norm.bias', 'pika_llm.transformer.wte.weight', 'pika_llm.transformer.wpe.weight', 'pika_llm.transformer.h.0.decoder.ln_1.weight', 'pika_llm.transformer.h.0.decoder.ln_1.bias', 'pika_llm.transformer.h.0.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.0.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.0.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.0.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.0.decoder.ln_2.weight', 'pika_llm.transformer.h.0.decoder.ln_2.bias', 'pika_llm.transformer.h.0.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.0.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.0.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.0.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.1.decoder.ln_1.weight', 'pika_llm.transformer.h.1.decoder.ln_1.bias', 'pika_llm.transformer.h.1.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.1.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.1.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.1.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.1.decoder.ln_2.weight', 'pika_llm.transformer.h.1.decoder.ln_2.bias', 'pika_llm.transformer.h.1.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.1.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.1.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.1.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.2.decoder.ln_1.weight', 'pika_llm.transformer.h.2.decoder.ln_1.bias', 'pika_llm.transformer.h.2.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.2.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.2.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.2.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.2.decoder.ln_2.weight', 'pika_llm.transformer.h.2.decoder.ln_2.bias', 'pika_llm.transformer.h.2.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.2.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.2.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.2.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.3.decoder.ln_1.weight', 'pika_llm.transformer.h.3.decoder.ln_1.bias', 'pika_llm.transformer.h.3.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.3.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.3.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.3.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.3.decoder.ln_2.weight', 'pika_llm.transformer.h.3.decoder.ln_2.bias', 'pika_llm.transformer.h.3.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.3.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.3.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.3.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.4.decoder.ln_1.weight', 'pika_llm.transformer.h.4.decoder.ln_1.bias', 'pika_llm.transformer.h.4.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.4.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.4.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.4.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.4.decoder.ln_2.weight', 'pika_llm.transformer.h.4.decoder.ln_2.bias', 'pika_llm.transformer.h.4.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.4.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.4.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.4.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.5.decoder.ln_1.weight', 'pika_llm.transformer.h.5.decoder.ln_1.bias', 'pika_llm.transformer.h.5.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.5.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.5.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.5.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.5.decoder.ln_2.weight', 'pika_llm.transformer.h.5.decoder.ln_2.bias', 'pika_llm.transformer.h.5.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.5.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.5.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.5.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.6.decoder.ln_1.weight', 'pika_llm.transformer.h.6.decoder.ln_1.bias', 'pika_llm.transformer.h.6.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.6.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.6.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.6.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.6.decoder.ln_2.weight', 'pika_llm.transformer.h.6.decoder.ln_2.bias', 'pika_llm.transformer.h.6.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.6.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.6.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.6.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.7.decoder.ln_1.weight', 'pika_llm.transformer.h.7.decoder.ln_1.bias', 'pika_llm.transformer.h.7.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.7.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.7.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.7.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.7.decoder.ln_2.weight', 'pika_llm.transformer.h.7.decoder.ln_2.bias', 'pika_llm.transformer.h.7.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.7.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.7.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.7.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.8.decoder.ln_1.weight', 'pika_llm.transformer.h.8.decoder.ln_1.bias', 'pika_llm.transformer.h.8.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.8.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.8.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.8.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.8.decoder.ln_2.weight', 'pika_llm.transformer.h.8.decoder.ln_2.bias', 'pika_llm.transformer.h.8.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.8.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.8.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.8.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.9.decoder.ln_1.weight', 'pika_llm.transformer.h.9.decoder.ln_1.bias', 'pika_llm.transformer.h.9.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.9.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.9.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.9.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.9.decoder.ln_2.weight', 'pika_llm.transformer.h.9.decoder.ln_2.bias', 'pika_llm.transformer.h.9.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.9.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.9.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.9.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.10.decoder.ln_1.weight', 'pika_llm.transformer.h.10.decoder.ln_1.bias', 'pika_llm.transformer.h.10.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.10.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.10.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.10.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.10.decoder.ln_2.weight', 'pika_llm.transformer.h.10.decoder.ln_2.bias', 'pika_llm.transformer.h.10.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.10.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.10.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.10.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.11.decoder.ln_1.weight', 'pika_llm.transformer.h.11.decoder.ln_1.bias', 'pika_llm.transformer.h.11.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.11.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.11.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.11.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.11.decoder.ln_2.weight', 'pika_llm.transformer.h.11.decoder.ln_2.bias', 'pika_llm.transformer.h.11.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.11.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.11.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.11.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.12.decoder.ln_1.weight', 'pika_llm.transformer.h.12.decoder.ln_1.bias', 'pika_llm.transformer.h.12.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.12.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.12.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.12.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.12.decoder.ln_2.weight', 'pika_llm.transformer.h.12.decoder.ln_2.bias', 'pika_llm.transformer.h.12.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.12.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.12.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.12.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.13.decoder.ln_1.weight', 'pika_llm.transformer.h.13.decoder.ln_1.bias', 'pika_llm.transformer.h.13.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.13.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.13.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.13.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.13.decoder.ln_2.weight', 'pika_llm.transformer.h.13.decoder.ln_2.bias', 'pika_llm.transformer.h.13.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.13.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.13.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.13.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.14.decoder.ln_1.weight', 'pika_llm.transformer.h.14.decoder.ln_1.bias', 'pika_llm.transformer.h.14.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.14.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.14.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.14.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.14.decoder.ln_2.weight', 'pika_llm.transformer.h.14.decoder.ln_2.bias', 'pika_llm.transformer.h.14.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.14.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.14.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.14.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.15.decoder.ln_1.weight', 'pika_llm.transformer.h.15.decoder.ln_1.bias', 'pika_llm.transformer.h.15.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.15.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.15.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.15.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.15.decoder.ln_2.weight', 'pika_llm.transformer.h.15.decoder.ln_2.bias', 'pika_llm.transformer.h.15.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.15.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.15.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.15.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.16.decoder.ln_1.weight', 'pika_llm.transformer.h.16.decoder.ln_1.bias', 'pika_llm.transformer.h.16.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.16.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.16.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.16.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.16.decoder.ln_2.weight', 'pika_llm.transformer.h.16.decoder.ln_2.bias', 'pika_llm.transformer.h.16.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.16.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.16.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.16.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.17.decoder.ln_1.weight', 'pika_llm.transformer.h.17.decoder.ln_1.bias', 'pika_llm.transformer.h.17.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.17.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.17.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.17.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.17.decoder.ln_2.weight', 'pika_llm.transformer.h.17.decoder.ln_2.bias', 'pika_llm.transformer.h.17.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.17.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.17.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.17.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.18.decoder.ln_1.weight', 'pika_llm.transformer.h.18.decoder.ln_1.bias', 'pika_llm.transformer.h.18.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.18.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.18.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.18.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.18.decoder.ln_2.weight', 'pika_llm.transformer.h.18.decoder.ln_2.bias', 'pika_llm.transformer.h.18.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.18.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.18.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.18.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.19.decoder.ln_1.weight', 'pika_llm.transformer.h.19.decoder.ln_1.bias', 'pika_llm.transformer.h.19.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.19.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.19.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.19.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.19.decoder.ln_2.weight', 'pika_llm.transformer.h.19.decoder.ln_2.bias', 'pika_llm.transformer.h.19.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.19.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.19.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.19.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.20.decoder.ln_1.weight', 'pika_llm.transformer.h.20.decoder.ln_1.bias', 'pika_llm.transformer.h.20.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.20.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.20.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.20.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.20.decoder.ln_2.weight', 'pika_llm.transformer.h.20.decoder.ln_2.bias', 'pika_llm.transformer.h.20.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.20.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.20.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.20.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.21.decoder.ln_1.weight', 'pika_llm.transformer.h.21.decoder.ln_1.bias', 'pika_llm.transformer.h.21.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.21.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.21.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.21.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.21.decoder.ln_2.weight', 'pika_llm.transformer.h.21.decoder.ln_2.bias', 'pika_llm.transformer.h.21.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.21.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.21.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.21.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.22.decoder.ln_1.weight', 'pika_llm.transformer.h.22.decoder.ln_1.bias', 'pika_llm.transformer.h.22.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.22.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.22.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.22.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.22.decoder.ln_2.weight', 'pika_llm.transformer.h.22.decoder.ln_2.bias', 'pika_llm.transformer.h.22.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.22.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.22.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.22.decoder.mlp.c_proj.bias', 'pika_llm.transformer.h.23.decoder.ln_1.weight', 'pika_llm.transformer.h.23.decoder.ln_1.bias', 'pika_llm.transformer.h.23.decoder.attn.c_attn.weight', 'pika_llm.transformer.h.23.decoder.attn.c_attn.bias', 'pika_llm.transformer.h.23.decoder.attn.c_proj.weight', 'pika_llm.transformer.h.23.decoder.attn.c_proj.bias', 'pika_llm.transformer.h.23.decoder.ln_2.weight', 'pika_llm.transformer.h.23.decoder.ln_2.bias', 'pika_llm.transformer.h.23.decoder.mlp.c_fc.weight', 'pika_llm.transformer.h.23.decoder.mlp.c_fc.bias', 'pika_llm.transformer.h.23.decoder.mlp.c_proj.weight', 'pika_llm.transformer.h.23.decoder.mlp.c_proj.bias', 'pika_llm.transformer.ln_f.weight', 'pika_llm.transformer.ln_f.bias', 'pika_llm.lm_head.weight']\n",
      "\u001B[32m2024-02-17 12:58:11.004\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mpika.utils.checkpoint_utils\u001B[0m:\u001B[36mload_from_checkpoint\u001B[0m:\u001B[36m38\u001B[0m - \u001B[1mpartial model loaded from checkpoint at ../model_checkpoints/self_pika_gpt2m_esm2m.ckpt\u001B[0m\n",
      "\u001B[32m2024-02-17 12:58:12.166\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mpika.datamodule.pika_datamodule\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m89\u001B[0m - \u001B[1mloading data from ../tests/assets/sample_data.pkl\u001B[0m\n",
      "\u001B[32m2024-02-17 12:58:12.169\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mpika.datamodule.pika_datamodule\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m95\u001B[0m - \u001B[1mloading splits from ../tests/assets/sample_split.csv\u001B[0m\n",
      "\u001B[32m2024-02-17 12:58:12.179\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mpika.datamodule.pika_datamodule\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m129\u001B[0m - \u001B[1mpreparing examples\u001B[0m\n",
      "\u001B[32m2024-02-17 12:58:12.180\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mpika.datamodule.pika_torch_datasets\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m27\u001B[0m - \u001B[1mpreparing train dataset\u001B[0m\n",
      "\u001B[32m2024-02-17 12:58:12.184\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mpika.datamodule.pika_torch_datasets\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m27\u001B[0m - \u001B[1mpreparing val dataset\u001B[0m\n",
      "\u001B[32m2024-02-17 12:58:12.188\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mpika.datamodule.pika_torch_datasets\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m71\u001B[0m - \u001B[1mpreparing val metrics dataset\u001B[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/elicarrami/opt/anaconda3/envs/pika/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "`Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "\u001B[32m2024-02-17 12:58:12.462\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mpika.main\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m92\u001B[0m - \u001B[1mwill save partial model checkpoints. Ensure this is intended.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'language_model': 'gpt2-medium',\n 'protein_model': 'esm2_t12_35M_UR50D',\n 'multimodal_strategy': 'self-pika',\n 'protein_layer_to_use': -1,\n 'perceiver_latent_size': 100,\n 'num_perceiver_layers': 4,\n 'multimodal_layers': [0],\n 'enable_gradient_checkpointing': False,\n 'lr': 0.0001,\n 'weight_decay': 0.0001,\n 'checkpoint': '../model_checkpoints/self_pika_gpt2m_esm2m.ckpt',\n 'schedulers': None}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pika(config)\n",
    "config[\"model\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /Users/elicarrami/PycharmProjects/Pika/notebooks/lightning_logs\n",
      "\u001B[32m2024-02-17 12:58:14.021\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mpika.datamodule.pika_torch_datasets\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m128\u001B[0m - \u001B[1mpreparing test dataset for catalytic activity\u001B[0m\n",
      "\u001B[32m2024-02-17 12:58:14.023\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mpika.datamodule.pika_torch_datasets\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m128\u001B[0m - \u001B[1mpreparing test dataset for taxonomy\u001B[0m\n",
      "/Users/elicarrami/opt/anaconda3/envs/pika/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c3c165140cd4232a92cda59cd170ca4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "   uniprot_id             subject  \\\n0  A0A068BGA5  catalytic activity   \n1  A0A075TMP8  catalytic activity   \n2  A0A068BGA5            taxonomy   \n3  A0A075TMP8            taxonomy   \n\n                                     expected_answer  \\\n0   benzoyl-CoA + ethanol = CoA + ethyl benzoate,...   \n1   3-hydroxybenzyl alcohol + O2 + reduced [NADPH...   \n2             Eukaryota, Viridiplantae, Streptophyta   \n3                          Eukaryota, Fungi, Dikarya   \n\n                                  generated_response  \n0   The protein catalyzes the conversion of acety...  \n1   The protein catalyzes the conversion of heme ...  \n2   This protein belongs to the Eukaryota domain,...  \n3   This protein belongs to the Eukaryota domain,...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uniprot_id</th>\n      <th>subject</th>\n      <th>expected_answer</th>\n      <th>generated_response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A0A068BGA5</td>\n      <td>catalytic activity</td>\n      <td>benzoyl-CoA + ethanol = CoA + ethyl benzoate,...</td>\n      <td>The protein catalyzes the conversion of acety...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A0A075TMP8</td>\n      <td>catalytic activity</td>\n      <td>3-hydroxybenzyl alcohol + O2 + reduced [NADPH...</td>\n      <td>The protein catalyzes the conversion of heme ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A0A068BGA5</td>\n      <td>taxonomy</td>\n      <td>Eukaryota, Viridiplantae, Streptophyta</td>\n      <td>This protein belongs to the Eukaryota domain,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A0A075TMP8</td>\n      <td>taxonomy</td>\n      <td>Eukaryota, Fungi, Dikarya</td>\n      <td>This protein belongs to the Eukaryota domain,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.biochem_react_benchmark()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
